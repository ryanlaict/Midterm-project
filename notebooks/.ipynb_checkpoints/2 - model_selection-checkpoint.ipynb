{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "This notebook should include preliminary and baseline modeling.\n",
    "- Try as many different models as possible.\n",
    "- Don't worry about hyperparameter tuning or cross validation here.\n",
    "- Ideas include:\n",
    "    - linear regression\n",
    "    - support vector machines\n",
    "    - random forest\n",
    "    - xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models and fit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from functions_variables import evaluate_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up data for Train/Test Split\n",
    "\n",
    "data = pd.read_csv('processed/chosen_features.csv')\n",
    "\n",
    "X = data.drop(columns = ['sold_price'], axis=1)\n",
    "y = data['sold_price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(953, 10)\n",
      "(239, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-6 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-6 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-6 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-6 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-6 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, objective=&#x27;reg:absoluteerror&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBRegressor</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, objective=&#x27;reg:absoluteerror&#x27;, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, objective='reg:absoluteerror', ...)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Regression models on entire feature set from chosen_features\n",
    "# Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Lasso Regression\n",
    "lasso_model = Lasso(alpha=1000)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_model = Ridge(alpha=100)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Support Vector Regression (SVR)\n",
    "svr_model_rbf = SVR(kernel='rbf')\n",
    "svr_model_rbf.fit(X_train, y_train)\n",
    "\n",
    "# Support Vector Regression (SVR)\n",
    "svr_model_linear = SVR(kernel='linear')\n",
    "svr_model_linear.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest Regression\n",
    "rf_model = RandomForestRegressor(n_estimators=100)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# XGBoost Regression\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:absoluteerror')\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6324244 2.5663652 2.572972  2.6406112 2.52705  ]\n"
     ]
    }
   ],
   "source": [
    "# Confirm XGBoost is working for me (Colter)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "print(y_pred[:5])  # Print first 5 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression:\n",
      "  MAE:  0.0269\n",
      "  RMSE: 0.0434\n",
      "  R²:   0.4712\n",
      "\n",
      "Lasso:\n",
      "  MAE:  0.0411\n",
      "  RMSE: 0.0601\n",
      "  R²:   -0.0130\n",
      "\n",
      "Ridge:\n",
      "  MAE:  0.0280\n",
      "  RMSE: 0.0462\n",
      "  R²:   0.4019\n",
      "\n",
      "SVR:\n",
      "  MAE:  0.0404\n",
      "  RMSE: 0.0576\n",
      "  R²:   0.0695\n",
      "\n",
      "SVR:\n",
      "  MAE:  2167.9196\n",
      "  RMSE: 3113.3436\n",
      "  R²:   -2715200478.6259\n",
      "\n",
      "RandomForestRegressor:\n",
      "  MAE:  0.0240\n",
      "  RMSE: 0.0385\n",
      "  R²:   0.5849\n",
      "\n",
      "XGBRegressor:\n",
      "  MAE:  0.0227\n",
      "  RMSE: 0.0371\n",
      "  R²:   0.6139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Prints MAE, RMSE, and R² for a given model\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"{model.__class__.__name__}:\")\n",
    "    print(f\"  MAE:  {mae:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  R²:   {r2:.4f}\\n\")\n",
    "\n",
    "models = [lr_model, lasso_model, ridge_model, svr_model_rbf, svr_model_linear, rf_model, xgb_model]\n",
    "\n",
    "for model in models:\n",
    "    evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             sqft       baths  price_reduced_amount         Stdev  \\\n",
      "count  953.000000  953.000000            953.000000    953.000000   \n",
      "mean     2.136004    2.341087              0.738266  46184.962547   \n",
      "std      0.055354    1.056389              1.086200  13376.262577   \n",
      "min      1.757132    0.000000              0.000000      0.000000   \n",
      "25%      2.100484    2.000000              0.000000  38611.333333   \n",
      "50%      2.132860    2.000000              0.000000  44154.000000   \n",
      "75%      2.169222    3.000000              2.253121  52978.000000   \n",
      "max      2.431579    9.000000              2.646038  86147.000000   \n",
      "\n",
      "                Mean  waterfront      garage  cost_of_living_housing  \\\n",
      "count     953.000000  953.000000  953.000000              953.000000   \n",
      "mean    66167.511885    0.023085    1.154250                1.858342   \n",
      "std     23645.397128    0.150252    1.217142                1.085560   \n",
      "min         0.000000    0.000000    0.000000                1.000000   \n",
      "25%     49943.000000    0.000000    0.000000                1.000000   \n",
      "50%     62953.000000    0.000000    1.000000                1.000000   \n",
      "75%     80217.500000    0.000000    2.000000                3.000000   \n",
      "max    172399.500000    1.000000   11.000000                4.000000   \n",
      "\n",
      "       total_population_category  cost_of_living_grocery  \n",
      "count                 953.000000              953.000000  \n",
      "mean                    1.994753                1.551941  \n",
      "std                     0.607182                0.497556  \n",
      "min                     1.000000                1.000000  \n",
      "25%                     2.000000                1.000000  \n",
      "50%                     2.000000                2.000000  \n",
      "75%                     2.000000                2.000000  \n",
      "max                     3.000000                2.000000  \n"
     ]
    }
   ],
   "source": [
    "print(X_train.describe())  # Check feature magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR:\n",
      "  MAE:  2167.9196\n",
      "  RMSE: 3113.3436\n",
      "  R²:   -2715200478.6259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rescale data to see if SVR Linear performs better/differently\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Run SVR Linear again to see if we get better evaluation metrics\n",
    "svr_model_linear.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate VR Linear model after rescaling\n",
    "evaluate_model(svr_model_linear, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 10\n",
      "Expanded polynomial features: 55\n",
      "\n",
      "Polynomial Regression Model - Training Metrics:\n",
      "MAE: 0.0236\n",
      "MSE: 0.0012\n",
      "R²: 0.6307\n",
      "RMSE: 0.0350\n",
      "\n",
      "Polynomial Regression Model - Testing Metrics:\n",
      "MAE: 0.0261\n",
      "MSE: 0.0019\n",
      "R²: 0.4692\n",
      "RMSE: 0.0435\n"
     ]
    }
   ],
   "source": [
    "# Polynomial Regression (using PolynomialFeatures)\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False) #Unable to run higher than 4 due to number of columns slowing down system\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = poly_model.predict(X_train_poly)\n",
    "y_test_pred = poly_model.predict(X_test_poly)\n",
    "\n",
    "# Evaluate model performance\n",
    "def evaluate_poly_model(y_true, y_pred, dataset_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    print(f\"\\nPolynomial Regression Model - {dataset_name} Metrics:\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Print feature expansion\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"Expanded polynomial features: {X_train_poly.shape[1]}\")\n",
    "\n",
    "# Print train & test metrics\n",
    "evaluate_poly_model(y_train, y_train_pred, \"Training\")\n",
    "evaluate_poly_model(y_test, y_test_pred, \"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 10\n",
      "Expanded polynomial features: 55\n",
      "\n",
      "Polynomial Regression Model - Training Metrics:\n",
      "MAE: 0.0237\n",
      "MSE: 0.0012\n",
      "R²: 0.6283\n",
      "RMSE: 0.0351\n",
      "\n",
      "Polynomial Regression Model - Testing Metrics:\n",
      "MAE: 0.0257\n",
      "MSE: 0.0018\n",
      "R²: 0.4822\n",
      "RMSE: 0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colte\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:215: LinAlgWarning: Ill-conditioned matrix (rcond=2.09288e-22): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "# Polynomial Regression (using Ridge and PolynomialFeatures)\n",
    "ridge_poly_model = Ridge(alpha=1.0)\n",
    "ridge_poly_model.fit(X_train_poly, y_train)\n",
    "y_train_pred_ridge = ridge_poly_model.predict(X_train_poly)\n",
    "y_test_pred_ridge = ridge_poly_model.predict(X_test_poly)\n",
    "\n",
    "# Evaluate model performance\n",
    "def evaluate_poly_model(y_true, y_pred, dataset_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    print(f\"\\nPolynomial Regression Model - {dataset_name} Metrics:\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Print feature expansion\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"Expanded polynomial features: {X_train_poly.shape[1]}\")\n",
    "\n",
    "# Print train & test metrics\n",
    "evaluate_poly_model(y_train, y_train_pred_ridge, \"Training\")\n",
    "evaluate_poly_model(y_test, y_test_pred_ridge, \"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 10\n",
      "Expanded polynomial features: 55\n",
      "\n",
      "Polynomial Regression Model - Training Metrics:\n",
      "MAE: 0.0238\n",
      "MSE: 0.0012\n",
      "R²: 0.6256\n",
      "RMSE: 0.0352\n",
      "\n",
      "Polynomial Regression Model - Testing Metrics:\n",
      "MAE: 0.0255\n",
      "MSE: 0.0018\n",
      "R²: 0.4918\n",
      "RMSE: 0.0426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colte\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:215: LinAlgWarning: Ill-conditioned matrix (rcond=1.86181e-21): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "# Polynomial Regression (using Ridge and PolynomialFeatures)\n",
    "ridge_poly_model = Ridge(alpha=10.0)  # Try higher alpha to see if that corrects the correlation error\n",
    "ridge_poly_model.fit(X_train_poly, y_train)\n",
    "y_train_pred_ridge = ridge_poly_model.predict(X_train_poly)\n",
    "y_test_pred_ridge = ridge_poly_model.predict(X_test_poly)\n",
    "\n",
    "# Evaluate model performance\n",
    "def evaluate_poly_model(y_true, y_pred, dataset_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    print(f\"\\nPolynomial Regression Model - {dataset_name} Metrics:\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Print feature expansion\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"Expanded polynomial features: {X_train_poly.shape[1]}\")\n",
    "\n",
    "# Print train & test metrics\n",
    "evaluate_poly_model(y_train, y_train_pred_ridge, \"Training\")\n",
    "evaluate_poly_model(y_test, y_test_pred_ridge, \"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Model: {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 10}\n",
      "Best CV score (RMSE): 0.03507018089726462\n",
      "Cross-validation RMSE: -0.0351\n",
      "\n",
      "Feature Importances from Best Random Forest Model:\n",
      "                     Feature  Importance\n",
      "0                       sqft    0.251431\n",
      "1                      baths    0.191614\n",
      "3                      Stdev    0.141637\n",
      "4                       Mean    0.119302\n",
      "7     cost_of_living_housing    0.099107\n",
      "6                     garage    0.080099\n",
      "8  total_population_category    0.040792\n",
      "9     cost_of_living_grocery    0.036833\n",
      "2       price_reduced_amount    0.034628\n",
      "5                 waterfront    0.004556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "# Hyperparameter grid \n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_tuned_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_tuned_model, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=20, \n",
    "    cv=5, \n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    random_state=42, \n",
    "    error_score=np.nan\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Random Forest Model:\", random_search.best_params_)\n",
    "print(\"Best CV score (RMSE):\", -random_search.best_score_)\n",
    "\n",
    "# Perform cross-validation on best model\n",
    "best_rf = random_search.best_estimator_\n",
    "cv_scores = cross_val_score(best_rf, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
    "print(f\"Cross-validation RMSE: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Print feature importances\n",
    "feature_importance = pd.DataFrame(\n",
    "    {'Feature': X_train.columns, 'Importance': best_rf.feature_importances_}\n",
    ").sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances from Best Random Forest Model:\")\n",
    "print(feature_importance.head(10))  # Show features in order of importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Model: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None}\n",
      "Best CV score (RMSE): 0.03499392832061003\n",
      "Cross-validation RMSE: -0.0350\n",
      "\n",
      "Feature Importances from Best Random Forest Model:\n",
      "                     Feature  Importance\n",
      "0                       sqft    0.259722\n",
      "1                      baths    0.171954\n",
      "3                      Stdev    0.145723\n",
      "4                       Mean    0.130743\n",
      "7     cost_of_living_housing    0.091603\n",
      "6                     garage    0.077256\n",
      "8  total_population_category    0.041568\n",
      "2       price_reduced_amount    0.039253\n",
      "9     cost_of_living_grocery    0.032161\n",
      "5                 waterfront    0.010019\n"
     ]
    }
   ],
   "source": [
    "# Same as above but with n_iter = 50 instead\n",
    "# Hyperparameter grid \n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_tuned_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_tuned_model, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=50, \n",
    "    cv=5, \n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    random_state=42, \n",
    "    error_score=np.nan\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Random Forest Model:\", random_search.best_params_)\n",
    "print(\"Best CV score (RMSE):\", -random_search.best_score_)\n",
    "\n",
    "# Perform cross-validation on best model\n",
    "best_rf = random_search.best_estimator_\n",
    "cv_scores = cross_val_score(best_rf, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
    "print(f\"Cross-validation RMSE: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Print feature importances\n",
    "feature_importance = pd.DataFrame(\n",
    "    {'Feature': X_train.columns, 'Importance': best_rf.feature_importances_}\n",
    ").sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances from Best Random Forest Model:\")\n",
    "print(feature_importance.head(10))  # Show features in order of importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best alpha for Lasso: {'alpha': 0.1}, Best RMSE: 0.0538\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best alpha for Ridge: {'alpha': 0.1}, Best RMSE: 0.0378\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {'alpha': [0.1, 1, 10, 100, 1000]}  # values to test for Alpha\n",
    "\n",
    "# For Lasso regression\n",
    "lasso = Lasso(max_iter=10000)\n",
    "lasso_grid_search = GridSearchCV(\n",
    "    lasso, param_grid, cv=5, scoring='neg_root_mean_squared_error', verbose=1\n",
    ")\n",
    "lasso_grid_search.fit(X_train, y_train)\n",
    "print(f\"Best alpha for Lasso: {lasso_grid_search.best_params_}, Best RMSE: {-lasso_grid_search.best_score_:.4f}\")\n",
    "\n",
    "# For Ridge regression\n",
    "ridge = Ridge(max_iter=10000)\n",
    "ridge_grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_root_mean_squared_error', verbose=1)\n",
    "ridge_grid_search.fit(X_train, y_train)\n",
    "print(f\"Best alpha for Ridge: {ridge_grid_search.best_params_}, Best RMSE: {-ridge_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Regression:\n",
      "MAE: 0.03246852538901924, MSE: 0.0023448147307753952, R2: 0.34316493458291075, RMSE: 0.04842328707115405\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Initialize model\n",
    "dt_model = DecisionTreeRegressor(random_state=10)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and calculate metrics\n",
    "y_pred = dt_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Print results\n",
    "print(\"Decision Tree Regression:\")\n",
    "print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}, RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors Regression:\n",
      "MAE: 0.030211400238612074, MSE: 0.002094152081288033, R2: 0.4133811506500997, RMSE: 0.04576190644289235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Initialize model\n",
    "knn_model = KNeighborsRegressor()\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate initial Score\n",
    "y_pred = knn_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Print results\n",
    "print(\"K-Nearest Neighbors Regression:\")\n",
    "print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}, RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider what metrics you want to use to evaluate success.\n",
    "- If you think about mean squared error, can we actually relate to the amount of error?\n",
    "- Try root mean squared error so that error is closer to the original units (dollars)\n",
    "- What does RMSE do to outliers?\n",
    "- Is mean absolute error a good metric for this problem?\n",
    "- What about R^2? Adjusted R^2?\n",
    "- Briefly describe your reasons for picking the metrics you use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Linear Regression\n",
      "Mean Absolute Error: 0.02691310537665663\n",
      "Mean Squared Error: 0.001887620875494322\n",
      "R-squared: 0.47123516200873816\n",
      "RMSE: 0.04344675909080356\n",
      "\n",
      "Model: Lasso Regression\n",
      "Mean Absolute Error: 0.04113705463015752\n",
      "Mean Squared Error: 0.0036161343869070984\n",
      "R-squared: -0.012960143676538971\n",
      "RMSE: 0.06013430291362076\n",
      "\n",
      "Model: Ridge Regression\n",
      "Mean Absolute Error: 0.027960560005742426\n",
      "Mean Squared Error: 0.002135299614376401\n",
      "R-squared: 0.40185480605957735\n",
      "RMSE: 0.04620930224939997\n",
      "\n",
      "Model: Support Vector Regression (RBF)\n",
      "Mean Absolute Error: 0.04039576579973451\n",
      "Mean Squared Error: 0.003321770827890505\n",
      "R-squared: 0.06949767484762792\n",
      "RMSE: 0.057634805698384244\n",
      "\n",
      "Model: Support Vector Regression (Linear)\n",
      "Mean Absolute Error: 2167.9195644615397\n",
      "Mean Squared Error: 9692908.337030414\n",
      "R-squared: -2715200478.625897\n",
      "RMSE: 3113.3435944383677\n",
      "\n",
      "Model: Random Forest Regression\n",
      "Mean Absolute Error: 0.024046381151803\n",
      "Mean Squared Error: 0.001481881285774493\n",
      "R-squared: 0.584891898491197\n",
      "RMSE: 0.03849521120573952\n",
      "\n",
      "Model: XGBoost Regression\n",
      "Mean Absolute Error: 0.02274769335897557\n",
      "Mean Squared Error: 0.001378375647109436\n",
      "R-squared: 0.6138861435593855\n",
      "RMSE: 0.037126481749681535\n",
      "\n",
      "Model: Nearest Neighbour\n",
      "Mean Absolute Error: 0.030211400238612074\n",
      "Mean Squared Error: 0.002094152081288033\n",
      "R-squared: 0.4133811506500997\n",
      "RMSE: 0.04576190644289235\n",
      "\n",
      "Model: Decision Tree\n",
      "Mean Absolute Error: 0.03246852538901924\n",
      "Mean Squared Error: 0.0023448147307753952\n",
      "R-squared: 0.34316493458291075\n",
      "RMSE: 0.04842328707115405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# gather evaluation metrics and compare results\n",
    "# Combine model information\n",
    "models = [lr_model, lasso_model, ridge_model, svr_model_rbf, svr_model_linear, rf_model, xgb_model, knn_model, dt_model]\n",
    "model_names = ['Linear Regression', 'Lasso Regression', 'Ridge Regression','Support Vector Regression (RBF)','Support Vector Regression (Linear)', 'Random Forest Regression', 'XGBoost Regression', 'Nearest Neighbour', 'Decision Tree']\n",
    "model_scores = []\n",
    "\n",
    "# model scores\n",
    "for model, name in zip(models, model_names):\n",
    "    y_pred = model.predict(X_test)            \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)  # Calculate RMSE\n",
    "    model_scores.append((name, mae, mse, r2, rmse))\n",
    "\n",
    "# Print results for each model\n",
    "for name, mae, mse, r2, rmse in model_scores:\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R-squared: {r2}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print() #Empty line between"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to compare against each evaluation metric. \n",
    "\n",
    "Mean Squared Error is probably the most volatile metric for this model because of the large differences in sold prices. \n",
    "\n",
    "The preference would go to R2 and RMSE. \n",
    "\n",
    "Because of that, Random Forest and XGBoost appear to be the most reliable models at this point with their nearly identical scores for RMSE and R2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection - STRETCH\n",
    "\n",
    "> **This step doesn't need to be part of your Minimum Viable Product (MVP), but its recommended you complete it if you have time!**\n",
    "\n",
    "Even with all the preprocessing we did in Notebook 1, you probably still have a lot of features. Are they all important for prediction?\n",
    "\n",
    "Investigate some feature selection algorithms (Lasso, RFE, Forward/Backward Selection)\n",
    "- Perform feature selection to get a reduced subset of your original features\n",
    "- Refit your models with this reduced dimensionality - how does performance change on your chosen metrics?\n",
    "- Based on this, should you include feature selection in your final pipeline? Explain\n",
    "\n",
    "Remember, feature selection often doesn't directly improve performance, but if performance remains the same, a simpler model is often preferrable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform feature selection \n",
    "# refit models\n",
    "# gather evaluation metrics and compare to the previous step (full feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Least Important Features (Random Forest):\n",
      "                     Feature  Importance\n",
      "9     cost_of_living_grocery    0.008297\n",
      "5                 waterfront    0.008726\n",
      "8  total_population_category    0.036647\n",
      "2       price_reduced_amount    0.038106\n",
      "6                     garage    0.046652\n"
     ]
    }
   ],
   "source": [
    "# Moving forward to test Linear Regression and Random Forest further\n",
    "# Determine features to drop, if any from RandomForest\n",
    "\n",
    "# Get feature importances from Random Forest\n",
    "rf_importance = rf_model.feature_importances_\n",
    "\n",
    "# Convert to DataFrame\n",
    "rf_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_importance\n",
    "}).sort_values(by='Importance', ascending=True)  # Sort least important first\n",
    "\n",
    "# Print least important features\n",
    "print(\"Least Important Features (Random Forest):\")\n",
    "print(rf_feature_importance.head(5))  # Change to see more/less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Least Important Features (Linear Regression):\n",
      "                  Feature   Coefficient\n",
      "4                    Mean  2.933398e-07\n",
      "3                   Stdev  3.250289e-07\n",
      "2    price_reduced_amount  1.437399e-03\n",
      "6                  garage  6.018279e-03\n",
      "9  cost_of_living_grocery  1.109896e-02\n"
     ]
    }
   ],
   "source": [
    "# Determine features to drop, if any from LinearRegression\n",
    "\n",
    "# Get absolute values of coefficients\n",
    "lr_coefficients = np.abs(lr_model.coef_)\n",
    "\n",
    "# Convert to DataFrame\n",
    "lr_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': lr_coefficients\n",
    "}).sort_values(by='Coefficient', ascending=True)  # Sort least important first\n",
    "\n",
    "# Print least important features\n",
    "print(\"Least Important Features (Linear Regression):\")\n",
    "print(lr_feature_importance.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 2 features\n",
    "drop_features_v1 = ['price_reduced_amount', 'garage']\n",
    "\n",
    "# Update train, test\n",
    "X_train_reduced_v1 = X_train.drop(columns=drop_features_v1)\n",
    "X_test_reduced_v1 = X_test.drop(columns=drop_features_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Comparison\n",
      "\n",
      "Linear Regression:\n",
      "R²: 0.4712  |  R² Reduced_v1: 0.4346\n",
      "RMSE: 0.0434  |  RMSE Reduced_v1: 0.0449\n",
      "\n",
      "Random Forest:\n",
      "R²: 0.5849  |  R² Reduced_v1: 0.5531\n",
      "RMSE: 0.0385  |  RMSE Reduced_v1: 0.0399\n"
     ]
    }
   ],
   "source": [
    "# Train new models for reduced_v1\n",
    "lr_model_reduced_v1 = LinearRegression()\n",
    "rf_model_reduced_v1 = RandomForestRegressor()\n",
    "\n",
    "lr_model_reduced_v1.fit(X_train_reduced_v1, y_train)\n",
    "rf_model_reduced_v1.fit(X_train_reduced_v1, y_train)\n",
    "\n",
    "# Generate predictions for the reduced_v1 feature set\n",
    "y_pred_lr_reduced_v1 = lr_model_reduced_v1.predict(X_test_reduced_v1)\n",
    "y_pred_rf_reduced_v1 = rf_model_reduced_v1.predict(X_test_reduced_v1)\n",
    "\n",
    "# Compute metrics for reduced_v1 models\n",
    "r2_lr_reduced_v1 = r2_score(y_test, y_pred_lr_reduced_v1)\n",
    "rmse_lr_reduced_v1 = np.sqrt(mean_squared_error(y_test, y_pred_lr_reduced_v1))\n",
    "\n",
    "r2_rf_reduced_v1 = r2_score(y_test, y_pred_rf_reduced_v1)\n",
    "rmse_rf_reduced_v1 = np.sqrt(mean_squared_error(y_test, y_pred_rf_reduced_v1))\n",
    "\n",
    "# Extract previous R² and RMSE for Linear Regression and Random Forest\n",
    "r2_lr_original = next(r2 for name, _, _, r2, _ in model_scores if name == \"Linear Regression\")\n",
    "r2_rf_original = next(r2 for name, _, _, r2, _ in model_scores if name == \"Random Forest Regression\")\n",
    "\n",
    "rmse_lr_original = next(rmse for name, _, _, _, rmse in model_scores if name == \"Linear Regression\")\n",
    "rmse_rf_original = next(rmse for name, _, _, _, rmse in model_scores if name == \"Random Forest Regression\")\n",
    "\n",
    "# Print side-by-side comparison\n",
    "print(\"\\nModel Performance Comparison\\n\")\n",
    "\n",
    "print(\"Linear Regression:\")\n",
    "print(f\"R²: {r2_lr_original:.4f}  |  R² Reduced_v1: {r2_lr_reduced_v1:.4f}\")\n",
    "print(f\"RMSE: {rmse_lr_original:.4f}  |  RMSE Reduced_v1: {rmse_lr_reduced_v1:.4f}\\n\")\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "print(f\"R²: {r2_rf_original:.4f}  |  R² Reduced_v1: {r2_rf_reduced_v1:.4f}\")\n",
    "print(f\"RMSE: {rmse_rf_original:.4f}  |  RMSE Reduced_v1: {rmse_rf_reduced_v1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 1 feature\n",
    "drop_features_v2 = ['price_reduced_amount']\n",
    "\n",
    "# Update train, test\n",
    "X_train_reduced_v2 = X_train.drop(columns=drop_features_v2)\n",
    "X_test_reduced_v2 = X_test.drop(columns=drop_features_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Comparison\n",
      "\n",
      "Linear Regression:\n",
      "R²: 0.4712  |  R² Reduced_v1: 0.4346  |  R² Reduced_v2: 0.4686\n",
      "RMSE: 0.0434  |  RMSE Reduced_v1: 0.0449  |  RMSE Reduced_v2: 0.0436\n",
      "\n",
      "Random Forest:\n",
      "R²: 0.5849  |  R² Reduced_v1: 0.5437  |  R² Reduced_v2: 0.5756\n",
      "RMSE: 0.0385  |  RMSE Reduced_v1: 0.0404  |  RMSE Reduced_v2: 0.0389\n"
     ]
    }
   ],
   "source": [
    "# Train new models for reduced_v2\n",
    "lr_model_reduced_v2 = LinearRegression()\n",
    "rf_model_reduced_v2 = RandomForestRegressor()\n",
    "\n",
    "lr_model_reduced_v2.fit(X_train_reduced_v2, y_train)\n",
    "rf_model_reduced_v2.fit(X_train_reduced_v2, y_train)\n",
    "\n",
    "# Generate predictions for the reduced_v2 feature set\n",
    "y_pred_lr_reduced_v2 = lr_model_reduced_v2.predict(X_test_reduced_v2)\n",
    "y_pred_rf_reduced_v2 = rf_model_reduced_v2.predict(X_test_reduced_v2)\n",
    "\n",
    "# Compute metrics for reduced_v2 models\n",
    "r2_lr_reduced_v2 = r2_score(y_test, y_pred_lr_reduced_v2)\n",
    "rmse_lr_reduced_v2 = np.sqrt(mean_squared_error(y_test, y_pred_lr_reduced_v2))\n",
    "\n",
    "r2_rf_reduced_v2 = r2_score(y_test, y_pred_rf_reduced_v2)\n",
    "rmse_rf_reduced_v2 = np.sqrt(mean_squared_error(y_test, y_pred_rf_reduced_v2))\n",
    "\n",
    "# Extract previous R² and RMSE for Linear Regression and Random Forest\n",
    "r2_lr_original = next(r2 for name, _, _, r2, _ in model_scores if name == \"Linear Regression\")\n",
    "r2_rf_original = next(r2 for name, _, _, r2, _ in model_scores if name == \"Random Forest Regression\")\n",
    "\n",
    "rmse_lr_original = next(rmse for name, _, _, _, rmse in model_scores if name == \"Linear Regression\")\n",
    "rmse_rf_original = next(rmse for name, _, _, _, rmse in model_scores if name == \"Random Forest Regression\")\n",
    "\n",
    "# Print side-by-side comparison\n",
    "print(\"\\nModel Performance Comparison\\n\")\n",
    "\n",
    "print(\"Linear Regression:\")\n",
    "print(f\"R²: {r2_lr_original:.4f}  |  R² Reduced_v1: {r2_lr_reduced_v1:.4f}  |  R² Reduced_v2: {r2_lr_reduced_v2:.4f}\")\n",
    "print(f\"RMSE: {rmse_lr_original:.4f}  |  RMSE Reduced_v1: {rmse_lr_reduced_v1:.4f}  |  RMSE Reduced_v2: {rmse_lr_reduced_v2:.4f}\\n\")\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "print(f\"R²: {r2_rf_original:.4f}  |  R² Reduced_v1: {r2_rf_reduced_v1:.4f}  |  R² Reduced_v2: {r2_rf_reduced_v2:.4f}\")\n",
    "print(f\"RMSE: {rmse_rf_original:.4f}  |  RMSE Reduced_v1: {rmse_rf_reduced_v1:.4f}  |  RMSE Reduced_v2: {rmse_rf_reduced_v2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features After RFE: ['sqft', 'baths', 'Stdev', 'Mean', 'cost_of_living_housing']\n",
      "Number of Features Selected: 5\n",
      "\n",
      "Model Performance with RFE-Selected Features\n",
      "\n",
      "Linear Regression:\n",
      "R²: 0.4045\n",
      "RMSE: 0.0461\n",
      "\n",
      "Random Forest:\n",
      "R²: 0.5216\n",
      "RMSE: 0.0413\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of the original feature set to avoid overwriting\n",
    "X_train_copy_rfe = X_train.copy()\n",
    "X_test_copy_rfe = X_test.copy()\n",
    "\n",
    "# Apply RFE to select top 5 features using Random Forest\n",
    "rfe = RFE(estimator=RandomForestRegressor(), n_features_to_select=5)\n",
    "X_train_rfe = rfe.fit_transform(X_train_copy_rfe, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_copy_rfe)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X_train.columns[rfe.support_]\n",
    "print(\"Selected Features After RFE:\", list(selected_features))\n",
    "print(f\"Number of Features Selected: {X_train_rfe.shape[1]}\")\n",
    "\n",
    "# Train new models on RFE-selected features\n",
    "lr_model_rfe = LinearRegression()\n",
    "rf_model_rfe = RandomForestRegressor()\n",
    "\n",
    "lr_model_rfe.fit(X_train_rfe, y_train)\n",
    "rf_model_rfe.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Generate predictions for RFE-selected features\n",
    "y_pred_lr_rfe = lr_model_rfe.predict(X_test_rfe)\n",
    "y_pred_rf_rfe = rf_model_rfe.predict(X_test_rfe)\n",
    "\n",
    "# Compute performance metrics\n",
    "r2_lr_rfe = r2_score(y_test, y_pred_lr_rfe)\n",
    "rmse_lr_rfe = np.sqrt(mean_squared_error(y_test, y_pred_lr_rfe))\n",
    "\n",
    "r2_rf_rfe = r2_score(y_test, y_pred_rf_rfe)\n",
    "rmse_rf_rfe = np.sqrt(mean_squared_error(y_test, y_pred_rf_rfe))\n",
    "\n",
    "# Print model performance after RFE\n",
    "print(\"\\nModel Performance with RFE-Selected Features\\n\")\n",
    "\n",
    "print(\"Linear Regression:\")\n",
    "print(f\"R²: {r2_lr_rfe:.4f}\")\n",
    "print(f\"RMSE: {rmse_lr_rfe:.4f}\\n\")\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "print(f\"R²: {r2_rf_rfe:.4f}\")\n",
    "print(f\"RMSE: {rmse_rf_rfe:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features After RFE: ['sqft', 'baths', 'waterfront', 'cost_of_living_housing', 'total_population_category']\n",
      "Number of Features Selected: 5\n",
      "\n",
      "Model Performance with RFE_LR-Selected Features\n",
      "\n",
      "Linear Regression:\n",
      "R²: 0.3878\n",
      "RMSE: 0.0467\n",
      "\n",
      "Random Forest:\n",
      "R²: 0.3005\n",
      "RMSE: 0.0500\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of the original feature set to avoid overwriting\n",
    "X_train_copy_rfe_lr = X_train.copy()\n",
    "X_test_copy_rfe_lr = X_test.copy()\n",
    "\n",
    "# Apply RFE to select top 5 features using Linear Regression\n",
    "rfe = RFE(estimator=LinearRegression(), n_features_to_select=5)\n",
    "X_train_rfe_lr = rfe.fit_transform(X_train_copy_rfe_lr, y_train)\n",
    "X_test_rfe_lr = rfe.transform(X_test_copy_rfe_lr)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X_train.columns[rfe.support_]\n",
    "print(\"Selected Features After RFE:\", list(selected_features))\n",
    "print(f\"Number of Features Selected: {X_train_rfe_lr.shape[1]}\")\n",
    "\n",
    "# Train new models on RFE-selected features\n",
    "lr_model_rfe_lr = LinearRegression()\n",
    "rf_model_rfe_lr = RandomForestRegressor()\n",
    "\n",
    "lr_model_rfe_lr.fit(X_train_rfe_lr, y_train)\n",
    "rf_model_rfe_lr.fit(X_train_rfe_lr, y_train)\n",
    "\n",
    "# Generate predictions for RFE-selected features\n",
    "y_pred_lr_rfe_lr = lr_model_rfe_lr.predict(X_test_rfe_lr)\n",
    "y_pred_rf_rfe_lr = rf_model_rfe_lr.predict(X_test_rfe_lr)\n",
    "\n",
    "# Compute performance metrics\n",
    "r2_lr_rfe_lr = r2_score(y_test, y_pred_lr_rfe_lr)\n",
    "rmse_lr_rfe_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr_rfe_lr))\n",
    "\n",
    "r2_rf_rfe_lr = r2_score(y_test, y_pred_rf_rfe_lr)\n",
    "rmse_rf_rfe_lr = np.sqrt(mean_squared_error(y_test, y_pred_rf_rfe_lr))\n",
    "\n",
    "# Print model performance after RFE\n",
    "print(\"\\nModel Performance with RFE_LR-Selected Features\\n\")\n",
    "\n",
    "print(\"Linear Regression:\")\n",
    "print(f\"R²: {r2_lr_rfe_lr:.4f}\")\n",
    "print(f\"RMSE: {rmse_lr_rfe_lr:.4f}\\n\")\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "print(f\"R²: {r2_rf_rfe_lr:.4f}\")\n",
    "print(f\"RMSE: {rmse_rf_rfe_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest using sqft, vaths, Stdev, Mean, cost_of_living_housing as our features has produced the most efficient and effective model according to RFE. Removing features made very little impact on the performance of the model, it simply improved simplicity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

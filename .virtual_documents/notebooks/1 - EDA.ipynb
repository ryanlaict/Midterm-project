





# (this is not an exhaustive list of libraries)
import pandas as pd
import numpy as np
import os
import json
import re
import ast
from pprint import pprint
from functions_variables import encode_tags
from functions_variables import encode_primary_photo
from functions_variables import encode_source
from functions_variables import categorize_cost_of_living
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split





# Directory containing JSON files
directory = "data" #Moved folder inside notebook folder

# Initialize an empty list to store data
data_list = []

# Loop through each JSON file in the directory
for filename in os.listdir(directory):
    if filename.endswith(".json"):
        file_path = os.path.join(directory, filename)
        try:
            with open(file_path, "r", encoding="utf-8") as file:
                data = json.load(file)  # Load JSON data

            # Check if expected keys exist before accessing them
            if isinstance(data, dict) and "data" in data and isinstance(data["data"], dict) and "results" in data["data"]:
                for record in data["data"]["results"]:
                    if isinstance(record, dict):  # Ensure record is a dictionary before processing
                        flat_record = record.copy()

                        # Extract state and city from location
                        flat_record["state"] = None
                        flat_record["city"] = None

                        if "location" in record and isinstance(record["location"], dict):
                            address = record["location"].get("address", {})
                            flat_record["state"] = address.get("state", None)
                            flat_record["city"] = address.get("city", None)
                            flat_record["postal_code"] = address.get("postal_code", None)

                        # Flatten description
                        if "description" in record and isinstance(record["description"], dict):
                            flat_record.update(record["description"])
                            del flat_record["description"]

                        # Flatten products (dictionary)
                        if "products" in record and isinstance(record["products"], dict):
                            flat_record.update(pd.json_normalize(record["products"]).to_dict(orient="records")[0])
                            del flat_record["products"]

                        # Append flattened record to list
                        data_list.append(flat_record)

        except json.JSONDecodeError as e:
            print(f"Error decoding JSON in file {filename}: {e}")

# Convert list of dictionaries into a DataFrame
df_combined = pd.DataFrame(data_list)

# Display or save the combined DataFrame
print(df_combined[["state", "city","postal_code"]].head())  # View extracted state and city columns
df_combined.to_csv("combined_data.csv", index=False)  # Save as CSV








df_combined.info()
df_combined.describe()


# load and concatenate data here
# drop or replace values as necessary

# Examine df_combined dtypes
df_combined.dtypes

# Create a list of categorical variables
categorical = df_combined.select_dtypes(include=['object']).columns

# Create a list of numerical variables
numerical = df_combined.select_dtypes(exclude=['object']).columns

print(categorical)
print(numerical)


# Columns to drop
columns_to_drop = ['last_update_date','permalink','location','sold_date','branding','flags','open_houses','status','lead_attributes' ,'property_id', 'photos', 'listing_id', 'list_price','other_listings', 'community', 'products',
       'virtual_tours','name']

# Drop the unneeded columns
df_dropped = df_combined.drop(columns=columns_to_drop)


df_dropped.head()


#Drop NA values for sales Price

df_cleaned = df_dropped.dropna(subset=['sold_price'])

#Fill NA Cities with most frequent City for each State

df_cleaned['city'] = df_cleaned['city'].fillna(
    df_dropped.groupby('state')['city'].transform(lambda x: x.value_counts().idxmax() if not x.dropna().empty else None)
)

#Replace NA with 0 for baths_3qtr, baths_full, and baths_half, baths_1qtr,garage, stories, beds

df_cleaned = df_cleaned.fillna({'baths_3qtr': 0, 'baths_full': 0, 'baths_half': 0, 'baths_1qtr': 0,'garage': 0, 'stories': 0, 'beds': 0})

#replace NA in type, sub_type with 'other'
df_cleaned = df_cleaned.fillna({'type': 'other', 'sub_type': 'other'})


# Save a copy of df_cleaned csv
df_cleaned.to_csv("cleaned_data.csv", index=False)


df_cleaned.head()








df_cleaned['tags'].head()


df_cleaned['source']


# Encode Tags on df_cleaned
df_encoded = encode_tags(df_cleaned)

# Run encode_source function
df_encoded = encode_source(df_encoded)

# Encode Primary Photo on df_encoded
df_encoded = encode_primary_photo(df_encoded)

# Convert to datetime format
df_encoded['list_date'] = pd.to_datetime(df_encoded['list_date'], errors='coerce')

# Format as MM-DD-YYYY
df_encoded['list_date'] = df_encoded['list_date'].dt.strftime('%m-%d-%Y')

df_encoded.head()








# perform train test split here

# Remove the '-' separating multi word cities and replacing with ' '
df_encoded['city'] = df_encoded['city'].str.replace('-', ' ', regex=False)

df_encoded[['city','state']].head(-40)

# Save a copy of df_encoded csv
df_encoded.to_csv("encoded_data.csv", index=False)

df_encoded.head()


# Save a copy of df_encoded csv
df_encoded.to_csv("encoded_data.csv", index=False)





# import, join and preprocess new data here
# Read city demographic data in df_city_data
df_city_data = pd.read_csv("us-cities-demographics.csv", sep=";")

# Pivot the df_city_data so 'Race' becomes columns and 'Count' is the value
df_city_pivot = df_city_data.pivot_table(
    index=['City', 'State'], 
    columns='Race', 
    values='Count', 
    aggfunc='sum'  # Ensure counts are summed if duplicates exist
).reset_index()

# Rename race columns for cleaner names
df_city_pivot.columns.name = None  # Remove MultiIndex column name
df_city_pivot = df_city_pivot.rename(columns=lambda x: x.replace(" ", "_") if isinstance(x, str) else x)  # Replace spaces with underscores

# Merge transformed df_city_pivot with df_encoded
df_merged = df_encoded.merge(
    df_city_pivot,
    left_on=['city', 'state'],
    right_on=['City', 'State'],
    how='left'
)

# Remove Race and City columns from df_city_data
df_city_data_cleaned = df_city_data.drop(columns=['Race', 'Count'])

# Merge df_merged with df_city_data_cleaned
df_merged = df_merged.merge(
    df_city_data_cleaned,
    left_on=['city','state'],
    right_on=['City','State'],
    how='left'
)

# Drop unnecessary columns after merging
df_merged.drop(columns=['City_y','City_x','State_y', 'State_x','State Code'], inplace=True)

# Drop duplicate rows
df_merged = df_merged.drop_duplicates()

# Display first few rows to check the result
df_merged.head()


df_merged.columns


# Save a copy of df_merged csv
df_merged.to_csv("merged_data.csv", index=False)


df_merged.head()


#Adding the cost of living dataset
cost_of_living_dataset = pd.read_csv("cost-of-living-index-by-state-2024.csv",sep=",")


cost_of_living_dataset.head()


#Merging the cost of living dataset to the merged dataset
df_merged_col = df_merged.merge(
    cost_of_living_dataset, 
    left_on='state',
    right_on='state',
    how='left'
    )

df_merged_col.head()


# Cleaning up column names
df_merged_col.columns = (
    df_merged_col.columns.str.strip() # Remove leading/trailing spaces
    .str.replace(r"[\[\]']", "", regex=True)  # Remove brackets and quotes
    .str.replace(" ", "_")    # Replace spaces with underscores
    .str.lower()              # Convert to lowercase
)


df_merged_col.columns


df_merged_col.shape


# Convert Bool columns to int
one_hot_cols = df_merged_col.select_dtypes(include=[bool]).columns  # Only select boolean columns
df_merged_col[one_hot_cols] = df_merged_col[one_hot_cols].astype(int)  # Convert True/False to 1/0

# Create list of column counts/sums
numeric_sums = df_merged_col.select_dtypes(include=[np.number]).sum()  # Get sum of each numeric column
sums_list = list(zip(numeric_sums.index, numeric_sums.values))  # Convert to list

for col, total in sums_list:
    print(f"{col}: {total}")


# Creating a list of merged columns to drop
additional_cols = ['costoflivingindexmisccostsindex', 'costoflivingindexhealthcostsindex','costoflivingindextransportationcostsindex', # merged columns we don't want
                   'matterport','baths_1qtr','baseball', 'baths_1qtr','baseball','boat_dock','coffer_ceiling','community_elevator','community_horse_facilities','courtyard_entry',
                   'detached_guest_house','dual_master_bedroom','fenced_courtyard','first_floor_master_bedroom','furniture','game_room','golf_course_view','greenbelt',
                   'greenhouse','guest_house','guest_parking','handicap_access','horse_facilities','indoor_basketball_court','kitchen_island','large_porch','low_hoa',
                   'master_bathroom','media_room','ocean_view','outdoor_kitchen','private_courtyard','private_parking','river_access','screen_porch','soccer','theater_room',
                   'two_kitchen','two_master_suites','volleyball','well_water','white_kitchen','wine_cellar','wrap_around_porch', # columns with very few listings (less than 10 listings)
                   'basketball','central_heat','horse_property','horse_stables', 'golf_course_lot_or_frontage', 'golf_course_view','low_hoa','solar_panels','tennis_court','views', #removing columns with potential multicolinearity
                   'lake_view','pond', #Removing these due to water_view column
                   'rv_parking', 'river_access','basketball','efficient', 
                   'golf_course_view','golf_course_lot_or_frontage' #Removing Columns with multiple names e.g view vs views
                   ]

#Dropping all unnecessary columns
df_merged_col_dropped = df_merged_col.drop(columns=additional_cols, errors = 'ignore')
df_merged_col_dropped = df_merged_col_dropped.loc[:, ~df_merged_col_dropped.columns.duplicated()] #Dropping duplicate columns


# Create list of column counts/sums after drops
numeric_sums_dropped = numeric_sums = df_merged_col_dropped.select_dtypes(include=[np.number]).sum()  # Get sum of each numeric column

# Identify columns occurring less than 1% of the time (1% of 1500 rows = 15 occurrences)
to_drop = numeric_sums_dropped[numeric_sums_dropped < 15].index

# Drop these columns
df_merged_col_dropped = df_merged_col_dropped.drop(columns=to_drop)  # Create a new DataFrame with dropped columns


# Get number of rows that include each tag
numeric_sums = df_merged_col_dropped.select_dtypes(include=[np.number]).sum()
sums_list = list(zip(numeric_sums.index, numeric_sums.values))

# Print columns and sums
for col, total in sums_list:
    print(f"{col}: {total}")


# Define the categorization function
def categorize_cost_of_living(score):
    if score < 100:
        return 'Low'
    elif 100 <= score <= 120:
        return 'Average'
    elif 120 < score <= 150:
        return 'High'
    else:
        return 'Very High'

# Apply the categorization function to COL columns
df_merged_col_dropped['cost_of_living_overall'] = df_merged_col_dropped['costoflivingindex2023'].apply(categorize_cost_of_living)
df_merged_col_dropped['cost_of_living_grocery'] = df_merged_col_dropped['costoflivingindexgrocerycostsindex'].apply(categorize_cost_of_living)
df_merged_col_dropped['cost_of_living_utility'] = df_merged_col_dropped['costoflivingindexutilitycostsindex'].apply(categorize_cost_of_living)
df_merged_col_dropped['cost_of_living_housing'] = df_merged_col_dropped['costoflivingindexhousingcostsindex'].apply(categorize_cost_of_living)

# Save df_merged_col_dropped to csv
df_merged_col_dropped.to_csv("col_merged_data.csv", index=False)

# print new data
df_merged_col_dropped.head()


#Importing us census bureau regions and divisions 
region_data = pd.read_csv(r'us census bureau regions and divisions (1).csv')
region_data.head()


df_merged_col_dropped.head()


df_merged_region = pd.merge(df_merged_col_dropped, region_data, left_on=['state'], right_on =['State'], how='left')
df_merged_region.drop(columns=['State', 'State Code'], inplace=True)


df_merged_region.head()


# Look for cities missing demographic data
missing_population_cities = df_merged_region[df_merged_region['total_population'].isna()]['city'].unique()
for col in missing_population_cities:
    print(col)


# Save a copy of updated df_merged csv
df_merged_region.to_csv("region_merged_data.csv", index=False)


df_income = pd.read_csv("kaggle_income.csv", encoding="latin1")


df_income.head()


df_income.shape


# Create new df that contains only required columns from df_income
cleaned_income_data = df_income.groupby(['Zip_Code'], as_index=False).agg({
    'State_Name': 'first',  # Keep the first state name (assuming it's the same per Zip_Code)
    'City': 'first',        # Keep the first city name (assuming it's the same per Zip_Code)
    'Mean': 'mean',         # Compute the average Mean for each Zip_Code
    'Median': 'mean',       # Compute the average Median for each Zip_Code
    'Stdev': 'mean'         # Compute the average Stdev for each Zip_Code
})

# print shape of cleaned_income_data
cleaned_income_data.shape


# Check Zip_Code dtype for merging
cleaned_income_data['Zip_Code'].dtype


df_merged_region.shape


# Check postal_code dtype for merging
df_merged_region['postal_code'].dtype


# Check for postal_code nulls
print(df_merged_region['postal_code'].isnull().sum())  # Count missing values


# Change dtype of postal_code to match Zip_Code
df_merged_region['postal_code'] = df_merged_region['postal_code'].astype('int64')

# Check postal_code dtype change
df_merged_region['postal_code'].dtype


# Merge df_merged_region with cleaned_income_data
df_merged_income = pd.merge(
    df_merged_region, 
    cleaned_income_data, 
    left_on=['postal_code'], 
    right_on =['Zip_Code'], 
    how='left'
)

# Drop repetitive columns
df_merged_income.drop(columns=['Zip_Code','City','State_Name'], inplace=True)


df_merged_income.head()


df_merged_income.shape


# Check for nulls in Mean, Median, Stdev
print(df_merged_income[['Mean', 'Median','Stdev']].isnull().sum())  # Count missing values


cleaned_income_data.head()


# Fill nulls in Median, Mean, Stdev with averages for those Cities
# Rename to match df_merged_income
cleaned_income_data.rename(columns={'City': 'city', 'State_Name': 'state'}, inplace=True)

# Compute mean values for each City & State_Name in cleaned_income_data
income_avg = cleaned_income_data.groupby(['city', 'state'])[['Mean', 'Median', 'Stdev']].mean()

# Fill NaN values in df_merged_income
df_merged_income['Mean'] = df_merged_income.apply(
    lambda row: income_avg.loc[(row['city'], row['state']), 'Mean']
    if pd.isna(row['Mean']) and (row['city'], row['state']) in income_avg.index else row['Mean'],
    axis=1
)

df_merged_income['Median'] = df_merged_income.apply(
    lambda row: income_avg.loc[(row['city'], row['state']), 'Median']
    if pd.isna(row['Median']) and (row['city'], row['state']) in income_avg.index else row['Median'],
    axis=1
)

df_merged_income['Stdev'] = df_merged_income.apply(
    lambda row: income_avg.loc[(row['city'], row['state']), 'Stdev']
    if pd.isna(row['Stdev']) and (row['city'], row['state']) in income_avg.index else row['Stdev'],
    axis=1
)

# Check for nulls in Mean, Median, Stdev
print(df_merged_income[['Mean', 'Median','Stdev']].isnull().sum())  # Count missing values


# Look at missing columns
df_merged_income[df_merged_income[['Mean', 'Median', 'Stdev']].isnull().any(axis=1)][['city', 'state', 'postal_code', 'Mean', 'Median', 'Stdev']]


# Drop rows with null income data
df_merged_income = df_merged_income.dropna(subset=['Median'])


# Confirm no more nulls in income data
print(df_merged_income['Median'].isnull().sum())  # Should print 0


# Save df_merged_income to csv
df_merged_income.to_csv("income_merged_data.csv", index=False)


for col in df_merged_income.columns:
    print(col)





df_merged_income = pd.read_csv("income_merged_data.csv")


# Get null counts for each column
null_counts = df_merged_income.isnull().sum()
null_counts = null_counts[null_counts > 0]  # Only show columns with missing values

# print null counts
print(null_counts)


df_merged_income['list_date'].dtype


# Rename the column
df_merged_income.rename(columns={'list_date': 'list_month'}, inplace=True)

# Rename list_date column as list_month
df_merged_income.rename(columns={'list_date': 'list_month'}, inplace=True)

# Convert to datetime explicitly, forcing MM-DD-YYYY format
df_merged_income['list_month'] = pd.to_datetime(df_merged_income['list_month'], format='%m-%d-%Y', errors='coerce')

# Check if datetime conversion worked
print(df_merged_income[['list_month']].head(10))  # Should show proper dates or NaT


# Extract month (will set NaT rows to NaN)
df_merged_income['list_month'] = df_merged_income['list_month'].dt.month

# Fill NaNs in list_month with 0 to represent unknown
df_merged_income['list_month'] = df_merged_income['list_month'].fillna(0).astype(int)

# Confirm unique values
print(df_merged_income['list_month'].unique())  # Should show 0, 1-12


# Fill Nulls
df_merged_income.loc[:, 'price_reduced_amount'] = df_merged_income['price_reduced_amount'].fillna(0)
df_merged_income.loc[:, 'year_built'] = df_merged_income['year_built'].fillna(df_merged_income['year_built'].mean())
df_merged_income.loc[:, 'median_age'] = df_merged_income['median_age'].fillna(df_merged_income['median_age'].mean())
df_merged_income.loc[:, 'baths'] = df_merged_income['baths'].fillna(df_merged_income['baths'].mean())
df_merged_income.loc[:, 'average_household_size'] = df_merged_income['average_household_size'].fillna(df_merged_income['average_household_size'].mean())

# Drop brand_name column
df_merged_income = df_merged_income.drop(columns=['brand_name'])


# Define columns to categorize
columns_to_categorize = [
    'american_indian_and_alaska_native', 'asian', 'black_or_african-american',
    'hispanic_or_latino', 'white', 'male_population', 'female_population',
    'total_population', 'number_of_veterans', 'foreign-born'
]

# Function to assign "Low", "Average", or "High" based on quartiles
def categorize_quartiles(value, q1, q3):
    if value <= q1:
        return 'Low'
    elif value >= q3:
        return 'High'
    else:
        return 'Average'

# Apply categorization to each column
for col in columns_to_categorize:
    q1 = df_merged_income[col].quantile(0.25)  # First quartile (Q1)
    q3 = df_merged_income[col].quantile(0.75)  # Third quartile (Q3)
    
    df_merged_income[col + '_category'] = df_merged_income[col].apply(lambda x: categorize_quartiles(x, q1, q3))


# Fill Nulls with "Average"
df_merged_income[columns_to_categorize] = df_merged_income[columns_to_categorize].fillna("Average")


# Create a dictionary to store category counts
category_counts = {}

# Loop through each categorized column
for col in columns_to_categorize:
    category_col = col + '_category'  # Corresponding category column

    # Count occurrences of each category (Low, Average, High)
    counts = df_merged_income[category_col].value_counts()

    # Store results in dictionary
    category_counts[col] = counts

# Convert dictionary to DataFrame for better readability
category_counts_df = pd.DataFrame(category_counts).T  # Transpose for better viewing

# Display the result
print(category_counts_df)


# Get null counts for each column
null_counts = df_merged_income.isnull().sum()
null_counts = null_counts[null_counts > 0]  # Only show columns with missing values

# print null counts
print(null_counts)


# Drop rows with nulls in lot_sqft or sqft
df_merged_income = df_merged_income.dropna(subset=['lot_sqft', 'sqft'])


# Get null counts for each column
null_counts = df_merged_income.isnull().sum()
null_counts = null_counts[null_counts > 0]  # Only show columns with missing values

# print null counts
print(null_counts)


# Examine df_merged_income dtypes
df_merged_income.dtypes

# Create a list of categorical variables
categorical = df_merged_income.select_dtypes(include=['object']).columns

# Create a list of numerical variables
numerical = df_merged_income.select_dtypes(exclude=['object']).columns

print(categorical)
print(numerical)


df_merged_income['type']


# Create lists of unique values in sub-types and types
unique_sub_types = df_merged_income['sub_type'].unique().tolist()
unique_types = df_merged_income['type'].unique().tolist()

# Print list of unique sub-types and types
print(unique_sub_types)
print(unique_types)


# One-hot encode type
df_merged_income = pd.get_dummies(df_merged_income, columns=['type'], prefix='type')


# Drop sub_type column
df_merged_income = df_merged_income.drop(columns=['sub_type'])


df_merged_income.head()


# Drop city column
df_merged_income = df_merged_income.drop(columns=['city'])

# Drop state column
df_merged_income = df_merged_income.drop(columns=['state'])


# # One-hot encode region
df_merged_income = pd.get_dummies(df_merged_income, columns=['Region'], prefix='Region')

# One-hot encode Division
df_merged_income = pd.get_dummies(df_merged_income, columns=['Division'], prefix='Division')


df_merged_income.head()


# Examine df_merged_income dtypes
df_merged_income.dtypes

# Create a list of categorical variables
categorical = df_merged_income.select_dtypes(include=['object']).columns

# Create a list of numerical variables
numerical = df_merged_income.select_dtypes(exclude=['object']).columns

print(categorical)
print(numerical)


# Drop columns that were categorized
df_merged_income = df_merged_income.drop(columns=columns_to_categorize)


# Define mapping for category values
category_mapping = {'Low': 1, 'Average': 2, 'High': 3, 'Very High': 4}

# List of columns to update
category_columns = [
    'cost_of_living_overall','cost_of_living_grocery', 'cost_of_living_utility',
    'cost_of_living_housing', 'american_indian_and_alaska_native_category',
    'asian_category', 'black_or_african-american_category',
    'hispanic_or_latino_category', 'white_category',
    'male_population_category', 'female_population_category',
    'total_population_category', 'number_of_veterans_category',
    'foreign-born_category'
]

# Apply mapping to each column
df_merged_income[category_columns] = df_merged_income[category_columns].replace(category_mapping)


# Examine df_merged_income dtypes
df_merged_income.dtypes

# Create a list of categorical variables
categorical = df_merged_income.select_dtypes(include=['object']).columns

# Create a list of numerical variables
numerical = df_merged_income.select_dtypes(exclude=['object']).columns

print(categorical)
print(numerical)


df_merged_income = df_merged_income.astype({col: int for col in df_merged_income.select_dtypes(include=[bool]).columns})


print(df_merged_income.isnull().sum().sum())  # Should be 0


df_merged_income.to_csv("cleaned_income_merged_data.csv", index=False)


# Create list of column counts/sums
numeric_sums = df_merged_income.select_dtypes(include=[np.number]).sum()  # Get sum of each numeric column
sums_list = list(zip(numeric_sums.index, numeric_sums.values))  # Convert to list

# Print columns and sums
for col, total in sums_list:
    print(f"{col}: {total}")


# Identify columns occurring less than 1% of the time (1% of 1500 rows = 15 occurrences)
to_drop = numeric_sums[numeric_sums < 15].index

# Drop these columns
df_merged_income = df_merged_income.drop(columns=to_drop)  # Create a new DataFrame with dropped columns


df_merged_income.shape


df_merged_income.head()


# Check for variables with low variance
low_variance = df_merged_income.var().sort_values()
print(low_variance.head(20))  # View lowest variance features


# Drop columns with variance below 1%
df_merged_income = df_merged_income.drop(columns=low_variance[low_variance < 0.01].index)


# Check for columns that are closely correlated
# Compute correlation matrix (absolute values)
corr_matrix = df_merged_income.corr().abs()

# Mask the lower triangle to avoid duplicate values
high_corr_pairs = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1) > 0.9)

# Extract and display the actual correlated column pairs
correlated_columns = []
for col in high_corr_pairs.columns:
    for row in high_corr_pairs.index:
        if high_corr_pairs.loc[row, col] > 0.9:
            correlated_columns.append((row, col, high_corr_pairs.loc[row, col]))

# Sort by correlation value for better readability
correlated_columns = sorted(correlated_columns, key=lambda x: x[2], reverse=True)

# Print correlated column pairs in a clean format
print("\nHighly Correlated Column Pairs (r > 0.9):")
for col1, col2, corr_value in correlated_columns:
    print(f"- {col1} ↔ {col2} (r = {corr_value:.2f})")


# List of correlated features to drop
drop_correlated = [
    'male_population_category',
    'female_population_category',
    'costoflivingindex2023',
    'gated_community',
    'farm',
    'costoflivingindexhousingcostsindex',
    'costoflivingindex2023',
    'costoflivingindexgrocerycostsindex',
    'costoflivingindexutilitycostsindex'
]

df_merged_income = df_merged_income.drop(columns=drop_correlated)


# Check for columns that are closely correlated
# Compute correlation matrix (absolute values)
corr_matrix = df_merged_income.corr().abs()

# Mask the lower triangle to avoid duplicate values
high_corr_pairs = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1) > 0.9)

# Extract and display the actual correlated column pairs
correlated_columns = []
for col in high_corr_pairs.columns:
    for row in high_corr_pairs.index:
        if high_corr_pairs.loc[row, col] > 0.9:
            correlated_columns.append((row, col, high_corr_pairs.loc[row, col]))

# Sort by correlation value for better readability
correlated_columns = sorted(correlated_columns, key=lambda x: x[2], reverse=True)

# Print correlated column pairs in a clean format
print("\nHighly Correlated Column Pairs (r > 0.9):")
for col1, col2, corr_value in correlated_columns:
    print(f"- {col1} ↔ {col2} (r = {corr_value:.2f})")


df_merged_income.shape


df_merged_income.to_csv("cleaned_income_merged_data.csv")


# Split Data into train and test
from sklearn.model_selection import train_test_split

# Define features (X) and target variable (y)
X = df_merged_income.drop(columns=['sold_price'])  
y = df_merged_income['sold_price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=10)


from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators = 100, random_state=10)
rf.fit(X_train, y_train)

# Get feature importance
feature_importance = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)

print(feature_importance.head(25))


# Compute absolute Pearson correlations with target
correlation_with_target = df_merged_income.corr()['sold_price'].abs().sort_values(ascending=False)

# Print the top 25 correlated features (excluding the target itself)
print(correlation_with_target[1:26])  # Skip index 0 because it's the target column itself


# Convert feature importance and correlation values to DataFrames
importance_df = feature_importance.reset_index()
importance_df.columns = ['Feature', 'Importance']

correlation_df = correlation_with_target.reset_index()
correlation_df.columns = ['Feature', 'Correlation']

# Merge the two DataFrames on 'Feature' to find common ones
common_features_df = importance_df.merge(correlation_df, on='Feature')

# Compute the average of the two scores
common_features_df['Avg_Score'] = (common_features_df['Importance'] + common_features_df['Correlation']) / 2

# Sort by Avg_Score in descending order
common_features_df = common_features_df.sort_values(by='Avg_Score', ascending=False)

# Keep only the first 10 common features
common_features_df = common_features_df.head(10)

# Print the results
print("Top 10 Common Features (Ranked by Avg Score):")
print(common_features_df)





correlation_with_target


df = df_merged_income #Creating an Alias for faster plotting


correlated_values = ['baths','sqft','waterfront','garage','beds','stories'] #Top housing features 

#Creating a pair plot for all correlated Values

plt.figure(figsize=(12, 8))
sns.pairplot(df[correlated_values], diag_kind='kde')

plt.show()


num_columns = len(df.columns)
num_rows = (num_columns // 5) + (1 if num_columns % 5 != 0 else 0)

# Create a figure and a grid of subplots
fig, axes = plt.subplots(num_rows, 5, figsize=(15, num_rows * 4))

# Flatten the axes array to make it easier to loop through
axes = axes.flatten()

# Plot the distribution of each column
for i, column in enumerate(df.columns):
    sns.histplot(df[column], kde=True, ax=axes[i], color='skyblue') 
    axes[i].set_title(f'Distribution of {column}')

# Remove any empty subplots
for j in range(i+1, len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()


plt.figure(figsize=(10,5))
sns.barplot(x=df['list_month'], y=df['sold_price'])
plt.title("Sold Price Distribution by Listing Month")
plt.xlabel("Listing Month")
plt.ylabel("Sold Price")
plt.show()


plt.figure(figsize=(10,5))
sns.barplot(x=df['baths'], y=df['sold_price'])
plt.title("Sold Price Distribution by Listing Month")
plt.xlabel("Listing Month")
plt.ylabel("Sold Price")
plt.show()


plt.figure(figsize=(10,6))
sns.scatterplot(x=df['sqft'], y=df['sold_price'], alpha=0.5)
plt.title("Sold Price vs. Square Footage")
plt.xlabel("Square Footage")
plt.ylabel("Sold Price")
plt.show()


#Adjusting the plot to remove outliers
q_low = df['sqft'].quantile(0.01)
q_high = df['sqft'].quantile(0.99)
df_filtered = df[(df['sqft'] >= q_low) & (df['sqft'] <= q_high)]

plt.figure(figsize=(10,6))
sns.scatterplot(x=df_filtered['sqft'], y=df_filtered['sold_price'], alpha=0.5)
plt.title("Sold Price vs. Square Footage (Outliers Removed)")
plt.xlabel("Square Footage")
plt.ylabel("Sold Price")
plt.show()


#Comparing Relationship between highest correlated Values with target: Sold Price
variables_to_compare = ['sqft', 'lot_sqft', 'year_built', 'baths_full', 'baths_half', 'baths_3qtr', 'price_reduced_amount']
fig, axes = plt.subplots(nrows=len(variables_to_compare), figsize=(10, 5 * len(variables_to_compare)))
for i, var in enumerate(variables_to_compare):
    q_low = df[var].quantile(0.01)
    q_high = df[var].quantile(0.99)
    df_filtered = df[(df[var] >= q_low) & (df[var] <= q_high)]
    sns.scatterplot(x=df_filtered[var], y=df_filtered['sold_price'], alpha=0.5, ax=axes[i])
    sns.regplot(x=df_filtered[var], y=df_filtered['sold_price'], scatter=False, ax=axes[i], color='red')
    axes[i].set_title(f"Sold Price vs. {var} (Outliers Removed)")
    axes[i].set_xlabel(var)
    axes[i].set_ylabel("Sold Price")
plt.tight_layout()
plt.show()











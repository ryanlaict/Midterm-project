


from sklearn.model_selection import ParameterGrid
from sklearn.metrics import mean_squared_error
import numpy as np
from sklearn.model_selection import KFold

#Developing a Custom Cross-Validation Function

def custom_cross_validation(X_train, y_train, n_splits=5):
    '''Creates n_splits sets of training and validation folds using K-Fold cross-validation.

    Args:
      training_data (pd.DataFrame): The dataframe of features and target to be divided into folds.
      n_splits (int): The number of sets of folds to be created.

    Returns:
      tuple: A tuple of lists, where the first index is a list of the training folds, 
             and the second index is the corresponding validation folds.

    Example:
        >>> output = custom_cross_validation(train_df, n_splits=10)
        >>> output[0][0] # The first training fold
        >>> output[1][0] # The first validation fold
        >>> output[0][1] # The second training fold
        >>> output[1][1] # The second validation fold... etc.
    '''
    training_data = pd.concat([X_train, y_train], axis=1)
    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)  # Shuffle
    train_folds = []
    val_folds = []

    for train_index, val_index in kfold.split(training_data):
        train_fold = training_data.iloc[train_index] 
        val_fold = training_data.iloc[val_index]  
        train_folds.append(train_fold)
        val_folds.append(val_fold)

    return train_folds, val_folds


#Hyperparameter Search Function Creation 
from sklearn.metrics import mean_squared_error
import numpy as np
import itertools

def hyperparameter_search(training_folds, validation_folds, param_grid, model, scoring=mean_squared_error, higher_is_better=False):
    '''
    Performs a custom grid search for the best hyperparameters using k-fold validation.
    
    Args:
      training_folds (list): List of training fold dataframes (features and target concatenated).
      validation_folds (list): List of validation fold dataframes (features and target concatenated).
      param_grid (dict): Dictionary of possible hyperparameter values.
      model: Model that will be used to fit.
      scoring (function): Scoring function to evaluate model performance. Default is mean_squared_error.
      higher is better (bool): If True, higher scores are better; if False, lower scores are better. Default is False. This is to take into account R2 where the larger the score is better
      
    Returns:
      dict: Best hyperparameter settings based on the chosen metric.
    '''
    param_combinations = list(itertools.product(*param_grid.values()))
    param_names = list(param_grid.keys())
    
    best_score = float('-inf') if higher_is_better else float('inf')
    best_params = None
    
    for combination in param_combinations:
        params = dict(zip(param_names, combination))
        scores = []
        print(f"Testing parameters: {params}")
        
        for train_fold, val_fold in zip(training_folds, validation_folds):
            X_train, y_train = train_fold.iloc[:, :-1], train_fold.iloc[:, -1]
            X_val, y_val = val_fold.iloc[:, :-1], val_fold.iloc[:, -1]
            
            model.set_params(**params)
            model.fit(X_train, y_train)
            predictions = model.predict(X_val)
            
            score = scoring(y_val, predictions)
            scores.append(score)
        
        avg_score = np.mean(scores)
        print(f"Average Score: {avg_score:.4f}\n")
        
        if (higher_is_better and avg_score > best_score) or (not higher_is_better and avg_score < best_score):
            best_score = avg_score
            best_params = params
    
    print(f"Best Parameters: {best_params}")
    print(f"Best Score: {best_score:.4f}")
    
    return best_params





# perform tuning and cross validation here 
# using GridsearchCV/ RandomsearchCV (MVP)
# or your custom functions


import pandas as pd
from sklearn.model_selection import train_test_split


data = pd.read_csv('chosen_features.csv')
data.head()

X = data.drop(columns = ['sold_price'], axis=1) #Dropping Target
y = data['sold_price'] #Target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 


from functions_variables import custom_cross_validation

test = custom_cross_validation(X_train, y_train, n_splits=5)
test


train_folds, val_folds = custom_cross_validation(X_train, y_train, n_splits=5)
train_folds


from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score
from functions_variables import hyperparameter_search

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 20],  # None lets trees grow fully
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

test1 = hyperparameter_search(train_folds, val_folds, param_grid, RandomForestRegressor(), scoring=r2_score)
test1


import xgboost as xgb

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 20],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

test2 = hyperparameter_search(train_folds, val_folds, param_grid, xgb.XGBRegressor(objective='reg:absoluteerror'), scoring=r2_score)
test2





# save your best model here
import joblib

# Set best_params to test1 to assign params or test with best results
best_params = test1

# Ensure best_params contains all necessary keys
best_params.setdefault('random_state', 42)  # Ensure reproducibility
best_params.setdefault('n_jobs', -1)  # Use all available CPU cores for training

# Set best_model from hyperparamter_search
best_model = RandomForestRegressor(**best_params)
best_model.fit(X_train, y_train)  # Retrain with the best parameters

# Save best model
joblib.dump(best_model, 'models/best_random_forest.pkl')

# Load it later when needed
loaded_model = joblib.load('models/best_random_forest.pkl')





# Build pipeline here
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import numpy as np
import joblib
import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
import itertools

#Build Test Data
data = pd.read_csv('chosen_features.csv')

data_split = train_test_split(data, test_size=0.2, random_state=42)


#Test Params: 

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 20]
}

pipe = Pipeline([
    ("model", loaded_model)
])





# save your pipeline here



